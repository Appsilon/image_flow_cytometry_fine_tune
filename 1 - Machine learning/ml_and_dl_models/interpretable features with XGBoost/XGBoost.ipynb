{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training XGBoost\n",
    "\n",
    "Here we train the XGBoost model. We use our feature pre-selection pipeline and need to calculate the optimal `top-k` before hand. First let's load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scifAI.ml.auto_feature_selection import AutoFeatureSelection\n",
    "from scifAI.utils import list_of_dict_to_dict\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "import skimage\n",
    "plt.ion()   # interactive mode\n",
    "from skimage.transform import   rescale, resize \n",
    "import random\n",
    "#import umap\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "from skimage.transform import resize\n",
    "import h5py\n",
    "from skimage.exposure import rescale_intensity\n",
    "import seaborn as sns\n",
    "from umap import UMAP\n",
    "\n",
    "# Compare Algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, matthews_corrcoef, classification_report,confusion_matrix, accuracy_score, balanced_accuracy_score, cohen_kappa_score, f1_score,  precision_score, recall_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def classification_complete_report(estimator, X, y_true, labels = None ,plot = True ): \n",
    "    y_pred = estimator.predict(X)\n",
    "    y_pred_dummy = DummyClassifier().fit(X,y_true).predict(X)\n",
    "    print(classification_report(y_true, y_pred, labels = labels))\n",
    "    print(15*\"----\")\n",
    "    print(\"No information rate: %.2f\" % accuracy_score(y_true, y_pred_dummy) )\n",
    "    print(\"matthews correlation coeff: %.2f\" % (matthews_corrcoef(y_true, y_pred)) )\n",
    "    print(\"Cohen Kappa score: %.2f\" % (cohen_kappa_score(y_true, y_pred)) )\n",
    "    print(\"Accuracy: %.2f & balanced Accuracy: %.2f\" % (accuracy_score(y_true, y_pred), balanced_accuracy_score(y_true, y_pred)) )\n",
    "    print(\"macro F1 score: %.2f & micro F1 score: %.2f\" % (f1_score(y_true, y_pred, average = \"macro\"), f1_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(\"macro Precision score: %.2f & micro Precision score: %.2f\" % (precision_score(y_true, y_pred, average = \"macro\"), precision_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(\"macro Recall score: %.2f & micro Recall score: %.2f\" % (recall_score(y_true, y_pred, average = \"macro\"), recall_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(15*\"----\")\n",
    "    \n",
    "    if plot:\n",
    "        plot_confusion_matrix(estimator, X, y_true, labels = labels ,cmap = \"Greys\", xticks_rotation='vertical')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/pstore/data/DS4/synapse_data_features/metadata_subset.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7dd4155e9e44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/pstore/data/DS4/synapse_data_features/metadata_subset.csv.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m                 )\n\u001b[1;32m    647\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/pstore/data/DS4/synapse_data_features/metadata_subset.csv.gz'"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv(\"/projects/site/pred/data_science/year/2022/immunological_synapse/synapse_data_features/metadata_subset.csv.gz\")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_features = pd.read_csv(\"/pstore/data/DS4/synapse_data_features/df_features_subset.csv.gz\")\n",
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to avoid using irrelevant or meaningless features in our analysis, we exclude features which include these strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features_to_exclude = [\"Antibody\",\"Live-Dead\",\"CD18\",  \"_R2\",  \"_R3\",  \"_R8\", \"background\",\n",
    "                               \"moment\",\"percent\",\"mask_based_mean\",\"mask_based_min\",\"mask_based_max\",  \n",
    "                               \"RMS\",\"intensity_BF\",\"_R1_R2\",\"_R1_R3\",\"_R1_R4\",\"_R1_R5\",\"_R1_R6\",\"_R1_R7\",\n",
    "                               \"_R1_R8\",\"_R2_R1\",\"_R2_R3\",\"_R2_R4\",\"_R2_R5\",\"_R2_R6\",\"_R2_R7\",\"_R2_R8\",\n",
    "                               \"_R3_R1\",\"_R3_R2\",\"_R3_R4\",\"_R3_R5\",\"_R3_R6\",\"_R3_R7\",\"_R3_R8\",\n",
    "                               \"_R4_R1\",\"_R4_R2\",\"_R4_R3\",\"_R4_R5\",\"_R4_R6\",\"_R4_R7\",\"_R4_R8\",\n",
    "                               \"_R5_R1\",\"_R5_R2\",\"_R5_R3\",\"_R5_R4\",\"_R5_R8\",\"_R6_R1\",\"_R6_R2\",\n",
    "                               \"_R6_R3\",\"_R6_R4\",\"_R6_R5\",\"_R6_R7\",\"_R6_R8\",\"_R7_R1\",\"_R7_R2\",\n",
    "                               \"_R7_R3\",\"_R7_R4\",\"_R7_R5\",\"_R7_R6\",\"_R7_R8\",\"_R8_R1\",\"_R8_R2\",\n",
    "                               \"_R8_R3\",\"_R8_R4\",\"_R8_R5\",\"_R8_R6\",\"_R8_R7\",\"cell_distance\"]\n",
    "classification_indx = ~df_features.columns.str.contains( list_of_features_to_exclude[0]) \n",
    "\n",
    "for f_to_exclude in list_of_features_to_exclude:\n",
    "    classification_indx = classification_indx & ~df_features.columns.str.contains( f_to_exclude) \n",
    "    \n",
    "\n",
    "print(\"list of remained columns:\\n\", \n",
    "      df_features.columns[classification_indx],\n",
    "     \"\\n\",\n",
    "     \"so we have these many columns:\",\n",
    "     classification_indx.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we use the pre-assigned set of train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_interesting_classes = ['B_cell',  'T_cell', \n",
    "                        'T_cell_with_signaling',\n",
    "                        'T_cell_with_B_cell_fragments',\n",
    "                        'B_T_cell_in_one_layer',\n",
    "                        'Synapses_without_signaling', \n",
    "                        'Synapses_with_signaling',\n",
    "                        'No_cell_cell_interaction', \n",
    "                        'Multiplets'] \n",
    "\n",
    "indx = metadata.set.isin([ \"train\", \"validation\",\"test\" ])\n",
    "indx = indx & metadata.label.isin(set_of_interesting_classes)\n",
    "\n",
    "train_index = metadata[\"set\"] == \"train\"\n",
    "train_index = train_index & metadata.label.isin(set_of_interesting_classes)\n",
    "train_index = train_index[train_index].index\n",
    "\n",
    "validation_index = metadata[\"set\"] == \"validation\"\n",
    "validation_index = validation_index & metadata.label.isin(set_of_interesting_classes)\n",
    "validation_index = validation_index[validation_index].index\n",
    "\n",
    "test_index = metadata[\"set\"] == \"test\"\n",
    "test_index = test_index & metadata.label.isin(set_of_interesting_classes)\n",
    "test_index = test_index[test_index].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal top_k\n",
    "\n",
    "For finding the optimal top-k, we use the validation set to find the maximum achievable f1-macro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_k = list(reversed(range(1,classification_indx.sum(),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "f1_macro_validation = []\n",
    "for k in tqdm(list_of_k):\n",
    "    xgb = Pipeline([\n",
    "                ('StandardScaler', StandardScaler()), \n",
    "                ('AutoFeatureSelection', AutoFeatureSelection( top_k = k )),\n",
    "                 ('xgb', XGBClassifier(n_jobs = -1, \n",
    "                                       n_estimators = 100, \n",
    "                                       eval_metric = \"logloss\") )\n",
    "        ], verbose=False)\n",
    "\n",
    "    xgb.fit(df_features.loc[train_index.tolist() , classification_indx],  \n",
    "               metadata.loc[train_index.tolist()  , \"label\"] )\n",
    "        \n",
    "    f1_macro_validation.append(f1_score(metadata.loc[validation_index.tolist(), \"label\"],\n",
    "        xgb.predict(df_features.loc[validation_index.tolist(), \n",
    "                                    classification_indx]), average='macro'))\n",
    "    xgb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best f1-score is\",\n",
    "     round( np.array(f1_macro_validation).max(),2) ,\n",
    "     \"\\nwhich is reached with top-k=\",\n",
    "     list_of_k[np.array(f1_macro_validation).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "ax = sns.lineplot(x = list_of_k,\n",
    "                 y = f1_macro_validation,\n",
    "                 color = \"grey\")  \n",
    "\n",
    "ax.set_ylim(0.2,1.0)\n",
    "ax.axvline(list_of_k[np.array(f1_macro_validation).argmax()], \n",
    "           0.85,0.95,color = \"k\", ls = \"--\")\n",
    "\n",
    "plt.savefig('Supplementary Figure - top-k optimization.png', bbox_inches=\"tight\")\n",
    "plt.savefig('Supplementary Figure - top-k optimization.svg', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model\n",
    "\n",
    "Now we can train the model with the optimal top-k and all the available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = Pipeline([\n",
    "                ('StandardScaler', StandardScaler()), \n",
    "                ('AutoFeatureSelection', AutoFeatureSelection( top_k = top_k )),\n",
    "                 ('xgb', XGBClassifier(n_jobs = -1, \n",
    "                                       n_estimators = 100, \n",
    "                                       eval_metric = \"logloss\") )\n",
    "        ], verbose=True)\n",
    "\n",
    "xgb.fit(df_features.loc[train_index.tolist() + validation_index.tolist() , classification_indx],  \n",
    "               metadata.loc[train_index.tolist() + validation_index.tolist() , \"label\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.0)\n",
    "sns.set_style(\"ticks\")\n",
    "classification_complete_report(xgb, \n",
    "                               df_features.loc[test_index, classification_indx],\n",
    "                               metadata.loc[test_index, \"label\"], labels = set_of_interesting_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving the test set predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb.predict(df_features.loc[test_index, classification_indx])\n",
    "preds = pd.Series(preds).to_csv(\"XGBoost_preds.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
