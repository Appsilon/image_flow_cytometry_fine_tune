{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install /pstore/home/shetabs1/code/iflai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import skorch\n",
    "import torch\n",
    "from skorch import NeuralNetClassifier\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from scipy.stats import entropy\n",
    "import random\n",
    "import h5py\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import FeatureAgglomeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scifAI.dl.utils import train_validation_test_split\n",
    "from scifAI.dl.dataset import DatasetGenerator\n",
    "from scifAI.dl.utils import get_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scifAI.dl.models import PretrainedModel\n",
    "from skorch.callbacks import LRScheduler,Checkpoint,EpochScoring,EarlyStopping\n",
    "import torch.optim as optim\n",
    "from skorch.helper import predefined_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compare Algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import GaussianRandomProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, matthews_corrcoef, classification_report,confusion_matrix, accuracy_score, balanced_accuracy_score, cohen_kappa_score, f1_score,  precision_score, recall_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def classification_complete_report(y_true, y_pred ,labels = None  ): \n",
    "    print(classification_report(y_true, y_pred, labels = labels))\n",
    "    print(15*\"----\")\n",
    "    print(\"matthews correlation coeff: %.4f\" % (matthews_corrcoef(y_true, y_pred)) )\n",
    "    print(\"Cohen Kappa score: %.4f\" % (cohen_kappa_score(y_true, y_pred)) )\n",
    "    print(\"Accuracy: %.4f & balanced Accuracy: %.4f\" % (accuracy_score(y_true, y_pred), balanced_accuracy_score(y_true, y_pred)) )\n",
    "    print(\"macro F1 score: %.4f & micro F1 score: %.4f\" % (f1_score(y_true, y_pred, average = \"macro\"), f1_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(\"macro Precision score: %.4f & micro Precision score: %.4f\" % (precision_score(y_true, y_pred, average = \"macro\"), precision_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(\"macro Recall score: %.4f & micro Recall score: %.4f\" % (recall_score(y_true, y_pred, average = \"macro\"), recall_score(y_true, y_pred, average = \"micro\")) )\n",
    "    cm = confusion_matrix(y_true, y_pred,labels= labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "    print(15*\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"/pstore/data/DS4/synapse_data_features/metadata_subset.csv.gz\")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.set.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = metadata.condition.isin([\"-SEA\",\"+SEA\"])\n",
    "metadata = metadata.loc[indx, :].reset_index(drop = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_interesting_classes = ['B_cell',  \n",
    "                              'T_cell', \n",
    "                              'T_cell_with_signaling',\n",
    "                              'T_cell_with_B_cell_fragments',\n",
    "                              'B_T_cell_in_one_layer',\n",
    "                              'Synapses_without_signaling', \n",
    "                              'Synapses_with_signaling',\n",
    "                              'No_cell_cell_interaction',\n",
    "                              'Multiplets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"set\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_index = metadata[\"set\"] == \"unlabeled\"\n",
    "unlabeled_index = unlabeled_index[unlabeled_index].index\n",
    "\n",
    " \n",
    "labeled_index = metadata[\"label\"].isin(set_of_interesting_classes)\n",
    "\n",
    "train_index = metadata[\"set\"] == \"train\"\n",
    "train_index = train_index & labeled_index\n",
    "train_index = train_index[train_index].index\n",
    "\n",
    "validation_index = metadata[\"set\"] == \"validation\"\n",
    "validation_index = validation_index & labeled_index\n",
    "validation_index = validation_index[validation_index].index\n",
    "\n",
    "test_index = metadata[\"set\"] == \"test\"\n",
    "test_index = test_index & labeled_index\n",
    "test_index = test_index[test_index].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_map = dict()\n",
    "for i, cl in enumerate(set_of_interesting_classes):\n",
    "    label_map[cl] = i\n",
    "\n",
    "label_map['-1'] = -1\n",
    "label_map[-1] = -1\n",
    "\n",
    "print(\"Number of labeled images\",labeled_index.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = {\n",
    "     \"Ch1\": (\"Greys\", \"BF\"),  \n",
    "     \"Ch2\": (\"Greens\", \"Antibody\"),\n",
    "     \"Ch3\": (\"Reds\", \"CD18\"),\n",
    "     \"Ch4\": (\"Oranges\", \"F-Actin\"),\n",
    "     \"Ch6\": (\"RdPu\", \"MHCII\"),\n",
    "     \"Ch7\": (\"Purples\", \"CD3/CD4\"),\n",
    "     \"Ch11\": (\"Blues\", \"P-CD3zeta\"),\n",
    "     \"Ch12\": (\"Greens\", \"Live-Dead\")\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_size = 160\n",
    "selected_channels = [0,3,4,5,6]\n",
    "\n",
    "stats = dict()\n",
    "\n",
    "stats[\"lower_bound\"] = torch.tensor([756.5198,52.5891,45.7574,54.1436,39.0545])\n",
    "stats[\"upper_bound\"] = torch.tensor([835.3564,288.1139,271.3861,234.2525,90.0396])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler(object):\n",
    "    def __init__(self, min_in , max_in, min_out, max_out):\n",
    "        self.min_in = min_in.reshape(-1,1,1)\n",
    "        self.max_in = max_in.reshape(-1,1,1)\n",
    "        self.min_out = min_out\n",
    "        self.max_out = max_out\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        \n",
    "        tensor_ = (tensor - self.min_in)/(self.max_in - self.min_in)\n",
    "        tensor_ = tensor_*(self.max_out - self.min_out) + self.min_out\n",
    "        tensor_[tensor_<self.min_out]= self.min_out\n",
    "        tensor_[tensor_>self.max_out]= self.max_out\n",
    "        return tensor_\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(min_out={0}, max_out={1})'.format(self.min_out, self.max_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet152, resnet18, resnet34, resnet50\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet18Modified(nn.Module):\n",
    "    def __init__(self,  num_channels=3, num_classes=3, pretrained=True, progress=True, **kwargs):\n",
    "        super().__init__()\n",
    "        model = resnet18(pretrained=pretrained)\n",
    "        if num_channels != 3:\n",
    "            model.conv1 = nn.Conv2d(num_channels, 64, kernel_size=(7, 7),\n",
    "                                    stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):                \n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_modified = ResNet18Modified(len(selected_channels),\n",
    "                                     len(set_of_interesting_classes), \n",
    "                                     pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([ \n",
    "        MinMaxScaler(           min_in =  stats[\"lower_bound\"] , \n",
    "                                max_in =  stats[\"upper_bound\"] , \n",
    "                                min_out =  0. , \n",
    "                                max_out =  1.),\n",
    "        transforms.RandomResizedCrop(reshape_size, scale=(0.6, 1.0), ratio=(0.8, 1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        AddGaussianNoise(mean=0., std=0.05),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = DatasetGenerator(metadata.loc[train_index.tolist(),:], \n",
    "                                 reshape_size=reshape_size, \n",
    "                                 scaling_factor = 1.,\n",
    "                                label_map=label_map, \n",
    "                                 transform = train_transform,\n",
    "                                selected_channels=selected_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 40\n",
    "plt.imshow(train_dataset[i][0][0,:,:])\n",
    "print(train_dataset[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_transforms =  transforms.Compose([ \n",
    "        MinMaxScaler(           min_in =  stats[\"lower_bound\"] , \n",
    "                                max_in =  stats[\"upper_bound\"] , \n",
    "                                min_out =  0. , \n",
    "                                max_out =  1.),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        AddGaussianNoise(mean=0., std=0.01),\n",
    "])\n",
    "\n",
    "test_transforms =  transforms.Compose([ \n",
    "        MinMaxScaler(           min_in =  stats[\"lower_bound\"] , \n",
    "                                max_in =  stats[\"upper_bound\"] , \n",
    "                                min_out =  0. , \n",
    "                                max_out =  1.),\n",
    "])\n",
    "\n",
    "validation_dataset = DatasetGenerator(metadata.loc[validation_index.tolist(),:],\n",
    "                                 reshape_size=reshape_size, \n",
    "                                label_map=label_map,\n",
    "                                 scaling_factor = 1.,\n",
    "                                transform=validation_transforms, \n",
    "                                selected_channels=selected_channels)\n",
    "\n",
    "test_dataset = DatasetGenerator(metadata.loc[test_index.tolist(),:],\n",
    "                                 reshape_size=reshape_size, \n",
    "                                label_map=label_map,\n",
    "                                 scaling_factor = 1.,\n",
    "                                transform=test_transforms, \n",
    "                                selected_channels=selected_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LRScheduler(policy='ReduceLROnPlateau', factor=0.1, patience=10)\n",
    "#checkpoint = Checkpoint(f_params='resnet_18_imagenet_pretraiend_supervised_learning.pth', monitor='valid_f1_macro')\n",
    "\n",
    "epoch_scoring = EpochScoring(\"f1_macro\", \n",
    "                             name =  \"valid_f1_macro\", \n",
    "                             on_train = False,\n",
    "                             lower_is_better = False)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='valid_f1_macro', \n",
    "                               patience=50, \n",
    "                               threshold=0.0001, \n",
    "                               threshold_mode='rel', \n",
    "                               lower_is_better=False)\n",
    "\n",
    "model = NeuralNetClassifier(    \n",
    "    resnet18_modified, \n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    lr=0.01,\n",
    "    batch_size=32,\n",
    "    max_epochs=1000,\n",
    "    optimizer=optim.Adam,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__num_workers=5,\n",
    "    iterator_valid__shuffle=False,\n",
    "    iterator_valid__num_workers=1,\n",
    "    callbacks=[lr_scheduler,epoch_scoring, early_stopping],\n",
    "    train_split=predefined_split(validation_dataset),\n",
    "    device=\"cuda\",\n",
    "    warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(train_dataset, y = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.module.load_state_dict(torch.load('resnet_18_imagenet_pretraiend_supervised_learning.pth')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(validation_dataset)\n",
    "\n",
    "classification_complete_report([label_map[t] for t in validation_dataset.metadata.label], \n",
    "                               preds  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_interesting_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dataset)\n",
    "\n",
    "classification_complete_report([label_map[t] for t in test_dataset.metadata.label],  preds   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.Series(preds).to_csv(\"resnet18_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
